@article{stockPricePrediction,
title = {A novel interval dual convolutional neural network method for interval-valued stock price prediction},
journal = {Pattern Recognition},
volume = {145},
pages = {109920},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109920},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323006180},
author = {Manrui Jiang and Wei Chen and Huilin Xu and Yanxin Liu},
keywords = {Interval-valued time series, Interval-valued stock price, Stock price prediction, Relevant stock information, Convolutional neural network},
}

@article{arima,
author = {Manoharan, Poongodi and Vijayakumar, Vignesh and Chilamkurti, Naveen},
year = {2020},
month = {01},
pages = {396},
title = {Bitcoin price prediction using ARIMA model},
volume = {10},
journal = {International Journal of Internet Technology and Secured Transactions},
doi = {10.1504/IJITST.2020.108130}
}

@inproceedings{randomForestAndLSTM,
author = {Ma, Yilin and Han, Ruizhu and Fu, Xiaoling},
year = {2019},
month = {10},
pages = {126-130},
title = {Stock prediction based on random forest and LSTM neural network},
doi = {10.23919/ICCAS47443.2019.8971687}
}

@inbook{linearRegression,
author = {Jin, Xiwen and Yi, Chaoran},
year = {2022},
month = {12},
pages = {837-842},
title = {The Comparison of Stock Price Prediction Based on Linear Regression Model and Machine Learning Scenarios},
isbn = {978-94-6463-029-9},
doi = {10.2991/978-94-6463-030-5_82}
}

@inproceedings{MICN,
title={{MICN}: Multi-scale Local and Global Context Modeling for Long-term Series Forecasting},
author={Huiqiang Wang and Jian Peng and Feihu Huang and Jince Wang and Junhui Chen and Yifei Xiao},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=zt53IDUR1U}
}

@inproceedings{holtWinter,
author = {Awajan, Ahmad and Ismail, Mohd Tahir and Alwadi, Sadam},
year = {2019},
month = {12},
pages = {050006},
title = {Stock market forecasting using empirical mode decomposition with holt-winter},
volume = {2184},
journal = {AIP Conference Proceedings},
doi = {10.1063/1.5136394}
}


@article{linearRegressionMethod,
author = {Maulud, Dastan and Mohsin Abdulazeez, Adnan},
year = {2020},
month = {12},
pages = {140-147},
title = {A Review on Linear Regression Comprehensive in Machine Learning},
volume = {1},
journal = {Journal of Applied Science and Technology Trends},
doi = {10.38094/jastt1457}
}

@inproceedings{HoltWinter1,
author = {Lima, Susana and Gonçalves, A. Manuela and Costa, Marco},
year = {2019},
month = {12},
pages = {090003},
title = {Time series forecasting using Holt-Winters exponential smoothing: An application to economic data},
volume = {2186},
journal = {AIP Conference Proceedings},
doi = {10.1063/1.5137999}
}
@inproceedings{HoltWinter2,
  title={Time series Forecasting using Holt-Winters Exponential Smoothing},
  author={Prajakta S. Kalekar},
  year={2004},
  url={https://api.semanticscholar.org/CorpusID:13942871}
}
@inproceedings{HoltWinter3,
  title={Triple Exponential Smoothing},
  url={https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc435.htm}
}
@article{FourierTimeSeries,
author = {O., Awoyemi and Taiwo, Abass and Olatayo, Timothy},
year = {2024},
month = {04},
pages = {69-78},
title = {Trend-Fourier Time Series Regression Model for Secular-Cyclical Datasets},
volume = {7},
journal = {African Journal of Mathematics and Statistics Studies},
doi = {10.52589/AJMSS-SVX0BDPO}
}
@inproceedings{XGBoost,
author = {Chen, Tianqi and Guestrin, Carlos},
title = {XGBoost: A Scalable Tree Boosting System},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939785},
doi = {10.1145/2939672.2939785},
abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {785–794},
numpages = {10},
keywords = {large-scale machine learning},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@Article{RNN,
AUTHOR = {Noh, Seol-Hyun},
TITLE = {Analysis of Gradient Vanishing of RNNs and Performance Comparison},
JOURNAL = {Information},
VOLUME = {12},
YEAR = {2021},
NUMBER = {11},
ARTICLE-NUMBER = {442},
URL = {https://www.mdpi.com/2078-2489/12/11/442},
ISSN = {2078-2489},
ABSTRACT = {A recurrent neural network (RNN) combines variable-length input data with a hidden state that depends on previous time steps to generate output data. RNNs have been widely used in time-series data analysis, and various RNN algorithms have been proposed, such as the standard RNN, long short-term memory (LSTM), and gated recurrent units (GRUs). In particular, it has been experimentally proven that LSTM and GRU have higher validation accuracy and prediction accuracy than the standard RNN. The learning ability is a measure of the effectiveness of gradient of error information that would be backpropagated. This study provided a theoretical and experimental basis for the result that LSTM and GRU have more efficient gradient descent than the standard RNN by analyzing and experimenting the gradient vanishing of the standard RNN, LSTM, and GRU. As a result, LSTM and GRU are robust to the degradation of gradient descent even when LSTM and GRU learn long-range input data, which means that the learning ability of LSTM and GRU is greater than standard RNN when learning long-range input data. Therefore, LSTM and GRU have higher validation accuracy and prediction accuracy than the standard RNN. In addition, it was verified whether the experimental results of river-level prediction models, solar power generation prediction models, and speech signal models using the standard RNN, LSTM, and GRUs are consistent with the analysis results of gradient vanishing.},
DOI = {10.3390/info12110442}
}

@article{DeepLearning,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={Nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group},
  url={https://doi.org/10.1038/nature14539},
  doi={10.1038/nature14539},
  ISSN={1476-4687},
}
@misc{PredictStockCNNLSTM,
      title={Predicting Stock Market Time-Series Data using CNN-LSTM Neural Network Model}, 
      author={Aadhitya A and Rajapriya R and Vineetha R S and Anurag M Bagde},
      year={2023},
      eprint={2305.14378},
      archivePrefix={arXiv},
      primaryClass={q-fin.ST}
}

@article{ComparasionCNNLSTM,
author = {Iturrino, Carlos and Grasso, Francesco and Luchetta, Antonio and Piccirilli, Maria and Paolucci, Libero and Talluri, Giacomo},
year = {2020},
month = {09},
pages = {6755},
title = {A Comparison of Power Quality Disturbance Detection and Classification Methods Using CNN, LSTM and CNN-LSTM},
volume = {10},
journal = {Applied Sciences},
doi = {10.3390/app10196755}
}
@article{ComparisonGRU&ARIMA,
author = {Ridwan, Mochamad and Sadik, Kusman and Afendi, Farit},
year = {2024},
month = {01},
pages = {389 - 400},
title = {Comparison of ARIMA and GRU Models for High-Frequency Time Series Forecasting},
volume = {10},
journal = {Scientific Journal of Informatics},
doi = {10.15294/sji.v10i3.45965}
}
@article{LSTM,
author = {Li, Yan and Dai, Wei},
year = {2020},
month = {07},
pages = {},
title = {Bitcoin price forecasting method based on CNN-LSTM hybrid neural network model},
volume = {2020},
journal = {The Journal of Engineering},
doi = {10.1049/joe.2019.1203}
}
@inproceedings{MICN,
  title={MICN: Multi-scale Local and Global Context Modeling for Long-term Series Forecasting},
  author={Huiqiang Wang and Jian Peng and Feihu Huang and Jince Wang and Junhui Chen and Yifei Xiao},
  booktitle={International Conference on Learning Representations},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:259298592}
}